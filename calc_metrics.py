import argparse
import warnings
import json

import matplotlib.pyplot as plt
import pandas as pd
import sklearn
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

LABELS = ['de_dicto', 'de_re', 'neither']


def metrics(df, plot_confusion=False):
    targets = df['label_name']

    def get_choice_by_name(row):
        choice = row['choice']
        if pd.isna(choice):
            return 'neither'
        lbl_name = row['label_name']
        lbl_nr = row['label_nr']
        if lbl_nr == choice:
            return lbl_name
        else:
            return {'de_dicto': 'de_re', 'de_re': 'de_dicto'}[lbl_name]

    choices = df.apply(get_choice_by_name, axis=1)

    with warnings.catch_warnings():
        warnings.simplefilter('ignore', category=sklearn.exceptions.UndefinedMetricWarning)
        report_dct = classification_report(targets, choices, labels=LABELS, output_dict=True)
        report_str = classification_report(targets, choices, labels=LABELS)
        print(report_str)

    if 'micro avg' in report_dct:
        averages = list(val for key, val in report_dct['micro avg'].items() if key != 'support')
        assert all(val - averages[0] < 1e-6 for val in averages), averages
        del report_dct['micro avg']
        report_dct['accuracy'] = averages[0]

    matrix = confusion_matrix(targets, choices, labels=LABELS)
    print(matrix)
    for i, lbl in enumerate(LABELS):
        report_dct[lbl]['confusion'] = {l: int(matrix[i, j]) for j, l in enumerate(LABELS)}

    if plot_confusion:
        ConfusionMatrixDisplay.from_predictions(targets, choices)
        plt.show()

    print(f"{df['success'].sum()}/{len(df)} calls returned a usable, parseable result.")

    return report_dct


def inspect_errors(df):
    pd.set_option('display.max_colwidth', None)
    print('The completions that failed to parse: ')
    for i, row in df[~df['success']].iterrows():
        print('==== PROMPT ====')
        print(row['prompt'])
        print('\n==== COMPLETION ====')
        print(row['full_completion'])
        print('\n#################################################\n')


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='')
    parser.add_argument('--results_file', type=str)
    parser.add_argument('--print_failures', action='store_true')
    parser.add_argument('--plot_confusion', action='store_true')
    _args = parser.parse_args()

    _df = pd.read_json(_args.results_file, orient='records', lines=True)

    name, style = _args.results_file.replace('.jsonl', '').split('_results_')
    report = metrics(_df)
    report['name'] = name
    report['style'] = style
    if _args.print_failures:
        inspect_errors(_df)

    with open(_args.results_file.replace('results', 'metrics').replace('jsonl', 'json'), 'w') as fp:
        json.dump(report, fp)

