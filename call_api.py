import argparse
import os
import re

import pandas as pd
from pandarallel import pandarallel
from dotenv import load_dotenv
from tqdm.auto import tqdm
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential
)
import openai

from create_fragment import PROMPT_TEMPLATES

load_dotenv()

DEFAULT_PREFIXES = {
    r'gpt-3\.5-turbo': 'You are a helpful assistant.',
    r'oasst': 'Below are a series of dialogues between various people and an AI assistant. '
              'The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and '
              'humble-but-knowledgeable. The assistant is happy to help with almost anything, and will do '
              'its best to understand exactly what is needed. It also tries to avoid giving false or '
              'misleading information, and it caveats when it isn\'t entirely sure about the right answer. '
              'That said, the assistant is practical and really does its best, and doesn\'t let caution '
              'get too much in the way of being useful.'
}


def _prepare():
    tqdm.pandas()


def prepare_for_local_api():
    _prepare()
    pandarallel.initialize(progress_bar=True, nb_workers=1)
    openai.api_key = 'anything'
    openai.api_base = os.getenv('LOCAL_API_URL')


def prepare_for_official_openai_api():
    _prepare()
    pandarallel.initialize(progress_bar=True, nb_workers=4)
    openai.api_key = os.getenv('OPENAI_API_KEY')


@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def _openai_chat_complete(messages, model, prefix):
    responses = []
    api_messages = [
        {"role": "system", "content": prefix},
    ]
    for msg in messages:
        api_messages.append({"role": "user", "content": msg})
        response = openai.ChatCompletion.create(
            model=model,
            messages=api_messages,
            max_tokens=512,
            headers={'ngrok-skip-browser-warning': ''}
        )
        responses.append(response)
        api_messages.append(response['choices'][0]['message'])
    return responses


def query(prompts_df, model, prefix=None):
    if prefix is None:
        for model_regex, p in DEFAULT_PREFIXES.items():
            if re.match(model_regex, model):
                prefix = p
                break
    prompts_df['responses'] = prompts_df['messages'].parallel_apply(
        lambda messages: _openai_chat_complete(messages, model, prefix)
    )
    return prompts_df


def _process_completion(row):
    responses = row['responses']

    results = {}
    messages = []
    for i, response in enumerate(responses):
        message = response['choices'][0]['message']['content']
        messages.append(message)

        regex = PROMPT_TEMPLATES[row['prompt_style']][i]['regx']
        result = re.search(regex, message)
        if result:
            results.update(result.groupdict())

    return results


def process_completions(prompts_df):
    prompts_df['results'] = prompts_df.apply(_process_completion, axis=1)
    return prompts_df


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='')

    parser.add_argument('--fragment_file', type=str)
    parser.add_argument('--model', type=str, default=os.getenv('MODEL'))
    parser.add_argument('--api', type=str, choices=['openai_official', 'local'], default=os.getenv('API'))
    parser.add_argument('--prefix', type=str, default=None)
    parser.add_argument('--output_dir', type=str, default='outputs/')
    parser.add_argument('--sample_n', type=int, default=100)
    parser.add_argument('--sample_seed', type=int, default=0)
    parser.add_argument('--reuse_completions', action='store_true')
    parser.add_argument('--overwrite', action='store_true')

    _args = parser.parse_args()

    def _name(name='results'):
        return os.path.basename(_args.fragment_file).replace('prompts', name)

    _completion_path = os.path.join(_args.output_dir, _name('completions'))
    _results_path = os.path.join(_args.output_dir, _name('results'))

    if not _args.overwrite and os.path.exists(_results_path):
        print('Not overwriting existing results, exiting.')
        exit()

    if not (_args.reuse_completions and os.path.exists(_completion_path)):
        if _args.api == 'openai_official':
            prepare_for_official_openai_api()
        elif _args.api == 'local':
            prepare_for_local_api()
        else:
            raise ValueError('unsupported api')

        # read fragment and sample n instances
        _df = pd.read_json(_args.fragment_file, lines=True, orient='records')
        _a = _df[_df['label_name'] == 'de_dicto'].sample(n=_args.sample_n//2, random_state=_args.sample_seed)
        _b = _df[_df['label_name'] == 'de_re'].sample(n=_args.sample_n//2, random_state=_args.sample_seed)
        _df = pd.concat((_a, _b))

        # query
        _df = query(_df, _args.model, prefix=_args.prefix)
        _df.to_json(_completion_path, orient='records', lines=True)
    _df = pd.read_json(_completion_path, lines=True)

    # process completions
    _df = process_completions(_df)

    _df.to_json(_results_path, orient='records', lines=True)
